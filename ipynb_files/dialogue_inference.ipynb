{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec2d3d5-5633-4089-9834-6966be1530ef",
   "metadata": {},
   "source": [
    "# Import Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90f92b9-a689-41e3-97d0-b481552be49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211b201c-fe02-46db-b863-53f837ed7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved model and vocab path.\n",
    "STATE_PATH = \"../models/checkpoints/act_classifier/tuning/act_classifier_last_variant2.pt\"\n",
    "VOCAB_PATH = \"../data/processed/vocab.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6ec0ea-e147-43cf-b7bf-0d86e1eb1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from numeric act IDs to their corresponding act labels\n",
    "id2label = {\n",
    "            1: \"inform\",\n",
    "            2: \"question\",\n",
    "            3: \"directive\",\n",
    "            4: \"commissive\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b00150-91ae-48b0-99a3-9429950223f6",
   "metadata": {},
   "source": [
    "# LSTM-based Dialogue Act Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823828a9-7d6e-4538-97bc-8cf83c4c5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based Dialogue Act Classifier.\n",
    "class LSTMDialogueActClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, pad_idx,\n",
    "                 bidirectional=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=bidirectional, batch_first=True)\n",
    "        enc_out = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_out, output_dim)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)              \n",
    "        out, _ = self.lstm(emb)              \n",
    "        pooled = out.mean(dim=1)             \n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.fc(pooled)         \n",
    "        return self.logsoftmax(logits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dee63c2-fdd1-4595-bc84-7a39625b0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary from file\n",
    "with open(VOCAB_PATH, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "PAD_IDX = vocab[\"<PAD>\"]\n",
    "UNK_IDX = vocab[\"<UNK>\"]\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBED_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "BIDIR = False\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb09261-31b3-4fba-acf7-e2d70c8aac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_66316\\3015280435.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(STATE_PATH, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMDialogueActClassifier(\n",
       "  (embedding): Embedding(10948, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 256, batch_first=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
       "  (logsoftmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the LSTM-based Dialogue Act classifier\n",
    "act_model = LSTMDialogueActClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    pad_idx=PAD_IDX,\n",
    "    bidirectional=BIDIR,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "state = torch.load(STATE_PATH, map_location=device)\n",
    "act_model.load_state_dict(state)\n",
    "act_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80c7af-4da0-422f-ace6-1ba3a5753de4",
   "metadata": {},
   "source": [
    "### Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad3b525-0ce9-4ee1-9666-ad48814907c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess a text string into a list of tokens.\n",
    "def preprocess_text(s):\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        from nltk import word_tokenize\n",
    "\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"http\\S+\", \"\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z\\s]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        toks = word_tokenize(s)\n",
    "        return [lemmatizer.lemmatize(t) for t in toks if t and t not in stop_words]\n",
    "    except Exception:\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"http\\S+\", \"\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z\\s]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return [t for t in s.split(\" \") if t]\n",
    "\n",
    "# Function to split a text string into sentences.\n",
    "def sent_split(s):\n",
    "    try:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        return [x.strip() for x in sent_tokenize(s) if x.strip()]\n",
    "    except Exception:\n",
    "        parts = re.split(r'(?<=[.!?])\\s+', s.strip())\n",
    "        return [x for x in parts if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc62052-7e3b-44f4-91b4-f3ad1a49b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length for model input\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Function to convert a list of tokens into a fixed-length tensor of token IDs.\n",
    "def encode(tokens):\n",
    "    ids = [vocab.get(t, UNK_IDX) for t in tokens[:MAX_LEN]]\n",
    "    ids += [PAD_IDX] * (MAX_LEN - len(ids))\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# Function to split a list of tokens into fixed-size chunks.\n",
    "def chunks(tokens, size=MAX_LEN):\n",
    "    return [tokens[i:i+size] for i in range(0, len(tokens), size)] or [[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a5d16-3ceb-46d4-9693-ae35cd239add",
   "metadata": {},
   "source": [
    "### Predicting emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1934f673-e02e-4ccf-845e-4cef1f5cb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the dialogue act of a given text input using the LSTMDialogueActClassifier.\n",
    "@torch.no_grad()\n",
    "def predict_act(text, agg=\"mean\", return_breakdown=True):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return {\"input\": text, \"pred\": None, \"prob\": 0.0, \"note\": \"Empty input\"}\n",
    "\n",
    "    sentences = sent_split(text)\n",
    "    if not sentences:\n",
    "        return {\"input\": text, \"pred\": None, \"prob\": 0.0, \"note\": \"No sentences found\"}\n",
    "\n",
    "    per_sentence = []\n",
    "    probs_matrix = []\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = preprocess_text(s)\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        chs = chunks(tokens, MAX_LEN)\n",
    "        batch = torch.stack([encode(c) for c in chs], dim=0).to(device)  # (B, T)\n",
    "\n",
    "        logp = act_model(batch)    # (B, C)\n",
    "        p = logp.exp()\n",
    "\n",
    "        if agg == \"max\":\n",
    "            p_sent = p.max(dim=0).values\n",
    "        else:\n",
    "            p_sent = p.mean(dim=0)\n",
    "\n",
    "        probs_matrix.append(p_sent.cpu().numpy())\n",
    "\n",
    "        if return_breakdown and len(sentences) > 1:\n",
    "            pred_idx = int(p_sent.argmax().item())\n",
    "            per_sentence.append({\n",
    "                \"text\": s,\n",
    "                \"pred\": id2label[pred_idx],\n",
    "                \"prob\": round(float(p_sent[pred_idx].item()), 2)\n",
    "            })\n",
    "\n",
    "    if not probs_matrix:\n",
    "        return {\"input\": text, \"pred\": None, \"prob\": 0.0, \"note\": \"No valid tokens after preprocessing\"}\n",
    "\n",
    "    P = np.stack(probs_matrix, axis=0)                 # (S, C)\n",
    "    P_agg = P.max(axis=0) if agg == \"max\" else P.mean(axis=0)\n",
    "    final_idx = int(P_agg.argmax())\n",
    "    final_prob = float(P_agg[final_idx])\n",
    "\n",
    "    out = {\n",
    "        \"input\": text,\n",
    "        \"pred_id\": final_idx,\n",
    "        \"pred\": id2label[final_idx],\n",
    "        \"prob\": round(final_prob, 2),\n",
    "        \"num_sentences\": len(sentences),\n",
    "        \"agg\": agg\n",
    "    }\n",
    "    if return_breakdown and per_sentence:\n",
    "        out[\"per_sentence\"] = per_sentence\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_project)",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
