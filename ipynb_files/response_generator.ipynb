{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1537e926-bf2d-47f5-86d3-0a16c6b264d1",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b330a3-e169-445a-9ae8-b60a9c426ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, pickle, re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a3447f-bd46-4b0a-8e8b-c64d165f71d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: mps\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device used:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec55d45-4841-4171-8e01-01059575b21c",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b144eb43-aa28-43a5-b8ae-a273297b4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paramaters\n",
    "ctx_turns = 2          \n",
    "max_src_len = 128\n",
    "max_tgt_len = 40\n",
    "batch_size = 32\n",
    "\n",
    "# model params\n",
    "embed_dimen = 256\n",
    "hidden_dimen = 256\n",
    "dropout = 0.3\n",
    "bidir_encoder = True   \n",
    "teacher_forcing = 0.5\n",
    "lr = 2e-3\n",
    "epochs = 10\n",
    "\n",
    "ckpt_dir = \"../models/checkpoints\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "best_ckpt = os.path.join(ckpt_dir, \"seq2seq_attn_best.pt\")\n",
    "last_ckpt = os.path.join(ckpt_dir, \"seq2seq_attn_last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b7d88e-1ec8-44cb-8fb9-67af124c953d",
   "metadata": {},
   "source": [
    "# Preprocess and vocab (extend with special/control tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2443b379-98f5-43a7-b829-91ac501d6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils.preprocess_dailydialog import preprocess_text\n",
    "except Exception:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    nltk.download('punkt', quiet=True); nltk.download('stopwords', quiet=True); nltk.download('wordnet', quiet=True)\n",
    "    _sw = set(stopwords.words('english')); _lem = WordNetLemmatizer()\n",
    "    def preprocess_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        toks = nltk.word_tokenize(text)\n",
    "        return [_lem.lemmatize(t) for t in toks if t not in _sw]\n",
    "\n",
    "with open(\"../data/processed/vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Add needed special tokens if missing\n",
    "SPECIALS = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<CTX_SEP>\",\n",
    "            \"<EMO_0>\", \"<EMO_1>\", \"<EMO_2>\", \"<EMO_3>\", \"<EMO_4>\", \"<EMO_5>\", \"<EMO_6>\",\n",
    "            \"<ACT_0>\", \"<ACT_1>\", \"<ACT_2>\", \"<ACT_3>\", \"<ACT_4>\"]\n",
    "for tok in SPECIALS:\n",
    "    if tok not in vocab:\n",
    "        vocab[tok] = len(vocab)\n",
    "\n",
    "pad_idx = vocab[\"<PAD>\"]\n",
    "unk_idx = vocab[\"<UNK>\"]\n",
    "bos_idx = vocab[\"<BOS>\"]\n",
    "eos_idx = vocab[\"<EOS>\"]\n",
    "sep_idx = vocab[\"<CTX_SEP>\"]\n",
    "\n",
    "def toks_to_ids(tokens, max_len):\n",
    "    ids = [vocab.get(t, unk_idx) for t in tokens][:max_len]\n",
    "    return ids\n",
    "\n",
    "def pad_to(ids, pad, max_len):\n",
    "    return ids + [pad] * (max_len - len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83af1cf-78b4-4bb7-a9c2-3cd161dcefb1",
   "metadata": {},
   "source": [
    "# Build context->response pairs from DailyDialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf27fed7-c4f5-4cb8-b6f8-f17d156641c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since daily_dialog couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/anaghar/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd (last modified on Sun Jun 15 03:11:15 2025).\n",
      "Using the latest cached version of the dataset since daily_dialog couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/anaghar/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd (last modified on Sun Jun 15 03:11:15 2025).\n",
      "Using the latest cached version of the dataset since daily_dialog couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/anaghar/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd (last modified on Sun Jun 15 03:11:15 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76052, 7069, 6740)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_pairs(split_name=\"train\", \n",
    "                ctx_turns=ctx_turns, \n",
    "                include_controls=True, \n",
    "                drop_dummy_act=True):\n",
    "    \n",
    "    ds = load_dataset(\"daily_dialog\", split=split_name)\n",
    "    pairs = []\n",
    "\n",
    "    for dialog, emos, acts in zip(ds[\"dialog\"], ds[\"emotion\"], ds[\"act\"]):\n",
    "        toks_list = [preprocess_text(utt) for utt in dialog]\n",
    "        for t in range(len(toks_list) - 1):\n",
    "            start = max(0, t - ctx_turns + 1)\n",
    "            ctx_utts = toks_list[start:t+1]     \n",
    "            resp_utts = toks_list[t+1] \n",
    "\n",
    "            # flatten context with separator\n",
    "            ctx = []\n",
    "            for u in ctx_utts:\n",
    "                if ctx: ctx.append(\"<CTX_SEP>\")\n",
    "                ctx.extend(u)\n",
    "\n",
    "            # optional control tokens (gold labels from HF)\n",
    "            if include_controls:\n",
    "                emo_id = emos[t+1] if emos[t+1] is not None else 0\n",
    "                act_id = acts[t+1] if acts[t+1] is not None else 0\n",
    "                if drop_dummy_act and act_id == 0:\n",
    "                    continue\n",
    "                ctx = [f\"<EMO_{emo_id}>\", f\"<ACT_{act_id}>\"] + ctx\n",
    "\n",
    "            pairs.append((ctx, resp_utts))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "train_pairs = build_pairs(\"train\")\n",
    "val_pairs   = build_pairs(\"validation\")\n",
    "test_pairs  = build_pairs(\"test\")\n",
    "len(train_pairs), len(val_pairs), len(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7fd1fe-e9f3-453c-b89b-f3c6702c7230",
   "metadata": {},
   "source": [
    "# Dataset & dynamic padding collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca2d48e-e9a2-44d3-8661-22f7a85c6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_toks, tgt_toks = self.pairs[idx]\n",
    "        # ids (truncate and add BOS/EOS to target)\n",
    "        src_ids = toks_to_ids(src_toks, max_src_len)\n",
    "        tgt_ids = [bos_idx] + toks_to_ids(tgt_toks, max_tgt_len-2) + [eos_idx]\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    srcs, tgts = zip(*batch)\n",
    "    src_lens = [len(s) for s in srcs]\n",
    "    tgt_lens = [len(t) for t in tgts]\n",
    "\n",
    "    max_src = max(src_lens); max_tgt = max(tgt_lens)\n",
    "    src_pad = torch.stack([F.pad(s, (0, max_src - len(s)), value=pad_idx) for s in srcs])\n",
    "    tgt_pad = torch.stack([F.pad(t, (0, max_tgt - len(t)), value=pad_idx) for t in tgts])\n",
    "\n",
    "    return src_pad, torch.tensor(src_lens), tgt_pad, torch.tensor(tgt_lens)\n",
    "\n",
    "train_ds = Seq2SeqDataset(train_pairs)\n",
    "val_ds   = Seq2SeqDataset(val_pairs)\n",
    "test_ds  = Seq2SeqDataset(test_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12874120-6767-4027-b52c-047f830841b7",
   "metadata": {},
   "source": [
    "# Model: Encoder, Luong-Attention, Decoder, Seq2Seq wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f206e0f-035c-4dfd-b07b-c738875828b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, bidirectional=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.bidir = bidirectional\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        # src: (B, T)\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        enc_outs, (h, c) = self.rnn(packed)\n",
    "        enc_outs, _ = nn.utils.rnn.pad_packed_sequence(enc_outs, batch_first=True)\n",
    "        if self.bidir:\n",
    "            # concat last forward/backward states\n",
    "            h = torch.cat([h[-2], h[-1]], dim=1).unsqueeze(0)\n",
    "            c = torch.cat([c[-2], c[-1]], dim=1).unsqueeze(0)\n",
    "        return enc_outs, (h, c)  # enc_outs for attention\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        assert enc_dim == dec_dim, \"For dot attention, enc_dim must equal dec_dim\"\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs, mask):\n",
    "        scores = torch.bmm(enc_outs, dec_hidden.unsqueeze(2)).squeeze(2)\n",
    "        scores = scores.masked_fill(~mask, -1e9)\n",
    "        attn_weights = F.softmax(scores, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outs).squeeze(1)  \n",
    "        return context, attn_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = LuongAttention(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward_step(self, input_tok, hidden, enc_outs, enc_mask):\n",
    "        # input_tok: (B,) token ids for current step\n",
    "        emb = self.dropout(self.embedding(input_tok.unsqueeze(1)))   \n",
    "        out, hidden = self.rnn(emb, hidden)                           \n",
    "        dec_h = out.squeeze(1)                                        \n",
    "        context, _ = self.attn(dec_h, enc_outs, enc_mask)             \n",
    "        logits = self.fc(torch.cat([dec_h, context], dim=1))\n",
    "        return logits, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, bidir_encoder=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        enc_out_dim = hidden_dim * (2 if bidir_encoder else 1)\n",
    "        self.encoder = Encoder(vocab_size, embed_dim, hidden_dim, pad_idx, bidirectional=bidir_encoder, dropout=dropout)\n",
    "        # For dot attention, make decoder hidden dim == enc_out_dim\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, enc_out_dim, pad_idx, dropout=dropout)\n",
    "        self.bridge_h = nn.Linear(enc_out_dim, enc_out_dim)\n",
    "        self.bridge_c = nn.Linear(enc_out_dim, enc_out_dim)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def make_src_mask(self, src_lens, max_time):\n",
    "        # returns (B, max_time) True for valid timesteps\n",
    "        B = src_lens.size(0)\n",
    "        rng = torch.arange(max_time, device=src_lens.device).unsqueeze(0).expand(B, -1)\n",
    "        return rng < src_lens.unsqueeze(1)\n",
    "\n",
    "    def forward(self, src, src_lens, tgt, teacher_forcing=0.5):\n",
    "\n",
    "        batch, Tt = tgt.size()\n",
    "        enc_outs, (h, c) = self.encoder(src, src_lens)\n",
    "        # init dec hidden with bridged encoder state\n",
    "        h0 = torch.tanh(self.bridge_h(h[-1])).unsqueeze(0)\n",
    "        c0 = torch.tanh(self.bridge_c(c[-1])).unsqueeze(0)\n",
    "\n",
    "        logits = []\n",
    "        inp = tgt[:, 0]\n",
    "        enc_mask = self.make_src_mask(src_lens, enc_outs.size(1)) \n",
    "\n",
    "        hidden = (h0, c0)\n",
    "        for t in range(1, Tt):\n",
    "            logit_t, hidden = self.decoder.forward_step(inp, hidden, enc_outs, enc_mask)\n",
    "            logits.append(logit_t.unsqueeze(1))\n",
    "            teacher = (random.random() < teacher_forcing)\n",
    "            inp = tgt[:, t] if teacher else logit_t.argmax(dim=1)\n",
    "\n",
    "        return torch.cat(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed0ff0-67f3-436d-a5bf-b89f88151533",
   "metadata": {},
   "source": [
    "# Train / validate (loss + BLEU) and save ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e931eab-3d23-4bf1-a1e9-8d69092a47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train: 6.2307 | val: 5.9549 | BLEU 0.0006\n",
      "Epoch 02 | train: 5.6418 | val: 5.8190 | BLEU 0.0015\n",
      "Epoch 03 | train: 5.2511 | val: 5.8145 | BLEU 0.0020\n",
      "Epoch 04 | train: 4.9879 | val: 5.8174 | BLEU 0.0026\n",
      "Epoch 05 | train: 4.7917 | val: 5.8703 | BLEU 0.0044\n",
      "Epoch 06 | train: 4.6410 | val: 5.8786 | BLEU 0.0049\n",
      "Epoch 07 | train: 4.5254 | val: 5.9146 | BLEU 0.0058\n",
      "Epoch 08 | train: 4.4234 | val: 5.9541 | BLEU 0.0070\n",
      "Epoch 09 | train: 4.3519 | val: 5.9635 | BLEU 0.0065\n",
      "Epoch 10 | train: 4.2757 | val: 5.9939 | BLEU 0.0077\n",
      "✓ Training done. Best and last checkpoints saved.\n"
     ]
    }
   ],
   "source": [
    "def seq_ce_loss(logits, tgt):\n",
    "\n",
    "    gold = tgt[:, 1:] \n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n",
    "                           gold.reshape(-1),\n",
    "                           ignore_index=pad_idx)\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, src, src_lens, max_len=max_tgt_len):\n",
    "    model.eval()\n",
    "    enc_outs, (h, c) = model.encoder(src, src_lens)\n",
    "    h0 = torch.tanh(model.bridge_h(h[-1])).unsqueeze(0)\n",
    "    c0 = torch.tanh(model.bridge_c(c[-1])).unsqueeze(0)\n",
    "    enc_mask = model.make_src_mask(src_lens, enc_outs.size(1))\n",
    "\n",
    "    batch = src.size(0)\n",
    "    inputs = torch.full((batch,), bos_idx, dtype=torch.long, device=src.device)\n",
    "    hidden = (h0, c0)\n",
    "\n",
    "    hyps = [[] for _ in range(batch)]\n",
    "    for _ in range(max_len-1):\n",
    "        logits, hidden = model.decoder.forward_step(inputs, hidden, enc_outs, enc_mask)\n",
    "        next_tok = logits.argmax(dim=1)\n",
    "        inputs = next_tok\n",
    "        for i in range(batch):\n",
    "            if next_tok[i].item() == eos_idx:\n",
    "                continue\n",
    "            hyps[i].append(int(next_tok[i].item()))\n",
    "    return hyps\n",
    "\n",
    "def evaluate_bleu(model, loader, max_len=max_tgt_len):\n",
    "    model.eval()\n",
    "    chencherry = SmoothingFunction().method3\n",
    "    refs, hyps = [], []\n",
    "    with torch.no_grad():\n",
    "        for src, src_lens, tgt, _ in loader:\n",
    "            src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n",
    "            preds = greedy_decode(model, src, src_lens, max_len)\n",
    "            for i in range(len(preds)):\n",
    "                ref_ids = tgt[i, 1:].tolist()  # drop BOS\n",
    "                if eos_idx in ref_ids:\n",
    "                    ref_ids = ref_ids[:ref_ids.index(eos_idx)]\n",
    "                elif pad_idx in ref_ids:\n",
    "                    ref_ids = ref_ids[:ref_ids.index(pad_idx)]\n",
    "                refs.append([[id_to_tok(x) for x in ref_ids if x != pad_idx]])\n",
    "                hyps.append([id_to_tok(x) for x in preds[i] if x not in (pad_idx, bos_idx, eos_idx)])\n",
    "    bleu = corpus_bleu(refs, hyps, smoothing_function=chencherry)\n",
    "    return bleu\n",
    "\n",
    "# small helpers\n",
    "inv_vocab = {i: t for t, i in vocab.items()}\n",
    "def id_to_tok(i): return inv_vocab.get(int(i), \"<UNK>\")\n",
    "\n",
    "model = Seq2Seq(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=embed_dimen,\n",
    "    hidden_dim=hidden_dimen,\n",
    "    pad_idx=pad_idx,\n",
    "    bidir_encoder=bidir_encoder,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "best_val = float(\"inf\"); best_bleu = 0.0\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total = 0.0; steps = 0\n",
    "    for src, src_lens, tgt, tgt_lens in train_loader:\n",
    "        src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, src_lens, tgt, teacher_forcing=teacher_forcing)\n",
    "        loss = seq_ce_loss(logits, tgt)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item(); steps += 1\n",
    "\n",
    "    val_loss = 0.0; vsteps = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src, src_lens, tgt, tgt_lens in val_loader:\n",
    "            src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n",
    "            logits = model(src, src_lens, tgt, teacher_forcing=0.0)\n",
    "            loss = seq_ce_loss(logits, tgt)\n",
    "            val_loss += loss.item(); vsteps += 1\n",
    "    val_loss /= max(1, vsteps)\n",
    "\n",
    "    # optional BLEU each epoch\n",
    "    bleu = evaluate_bleu(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train: {total/max(1,steps):.4f} | val: {val_loss:.4f} | BLEU {bleu:.4f}\")\n",
    "\n",
    "    # save best by BLEU first, else by val loss\n",
    "    improved = False\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu; improved = True\n",
    "    elif val_loss < best_val:\n",
    "        improved = True\n",
    "    if improved:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), best_ckpt)\n",
    "\n",
    "torch.save(model.state_dict(), last_ckpt)\n",
    "print(\"✓ Training done. Best and last checkpoints saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157e743-0036-4e3b-a38c-7bcfefa3bac7",
   "metadata": {},
   "source": [
    "# Greedy decode + quick samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91085608-f37a-4af8-b7cb-e878de51b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX: <EMO_0> <ACT_1> good morning sir bank near\n",
      "REF: one block away\n",
      "GEN: yes morning\n",
      "---\n",
      "CTX: <EMO_0> <ACT_3> good morning sir bank near <CTX_SEP> one block away\n",
      "REF: well thats <UNK> change money\n",
      "GEN: yes sir\n",
      "---\n",
      "CTX: <EMO_0> <ACT_2> one block away <CTX_SEP> well thats <UNK> change money\n",
      "REF: surely course kind currency got\n",
      "GEN: oh\n",
      "---\n",
      "CTX: <EMO_0> <ACT_1> well thats <UNK> change money <CTX_SEP> surely course kind currency got\n",
      "REF: rib\n",
      "GEN: yes\n",
      "---\n",
      "CTX: <EMO_0> <ACT_2> surely course kind currency got <CTX_SEP> rib\n",
      "REF: much would like change\n",
      "GEN: would like euro\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def decode_ids(ids):\n",
    "    toks = [id_to_tok(i) for i in ids if i not in (pad_idx, bos_idx, eos_idx)]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "src, src_lens, tgt, _ = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n",
    "preds = greedy_decode(model, src, src_lens)\n",
    "\n",
    "for i in range(min(5, src.size(0))):\n",
    "    print(\"CTX:\", decode_ids(src[i].tolist()))\n",
    "    print(\"REF:\", decode_ids(tgt[i].tolist()[1:]))\n",
    "    print(\"GEN:\", decode_ids(preds[i]))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97938b24-ce96-40de-b4ab-b885df0eb562",
   "metadata": {},
   "source": [
    "# User-facing inference wrapper (with optional emotion/act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f7f953-ca02-4cd6-82cd-4ef5d8d176b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src, src_lens, max_len, beam_size=5, length_penalty=0.8, min_len=6):\n",
    "    model.eval()\n",
    "    enc_outs, (h, c) = model.encoder(src, src_lens)\n",
    "    h0 = torch.tanh(model.bridge_h(h[-1])).unsqueeze(0)\n",
    "    c0 = torch.tanh(model.bridge_c(c[-1])).unsqueeze(0)\n",
    "    enc_mask = model.make_src_mask(src_lens, enc_outs.size(1))\n",
    "\n",
    "    assert src.size(0) == 1, \"beam_search_decode expects batch=1\"\n",
    "    beams = [(0.0, [bos_idx], (h0, c0))]\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        new_beams = []\n",
    "        for logp, seq, hid in beams:\n",
    "            last = seq[-1]\n",
    "            if last == eos_idx and len(seq) > 1:\n",
    "                completed.append((logp, seq))\n",
    "                continue\n",
    "\n",
    "            # block EOS until min_len\n",
    "            block_eos = (len(seq) - 1) < min_len\n",
    "\n",
    "            inp = torch.tensor([last], device=src.device, dtype=torch.long)\n",
    "            logits, new_hid = model.decoder.forward_step(inp, hid, enc_outs, enc_mask)\n",
    "            probs = F.log_softmax(logits, dim=-1).squeeze(0)\n",
    "            if block_eos:\n",
    "                probs[eos_idx] = -1e9\n",
    "\n",
    "            topk_logp, topk_idx = probs.topk(beam_size)\n",
    "            for k in range(beam_size):\n",
    "                new_beams.append((logp + float(topk_logp[k]),\n",
    "                                  seq + [int(topk_idx[k])],\n",
    "                                  new_hid))\n",
    "\n",
    "        # prune with length penalty\n",
    "        new_beams.sort(key=lambda x: x[0] / (len(x[1]) ** length_penalty), reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "    completed.extend([(lp, s) for lp, s, _ in beams])\n",
    "    completed.sort(key=lambda x: x[0] / (len(x[1]) ** length_penalty), reverse=True)\n",
    "    best = completed[0][1][1:]  # drop BOS\n",
    "    if eos_idx in best:\n",
    "        best = best[:best.index(eos_idx)]\n",
    "    return [best]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_decode(model, src, src_lens, max_len, temperature=0.9, top_k=50, top_p=0.9, min_len=6):\n",
    "    model.eval()\n",
    "    enc_outs, (h, c) = model.encoder(src, src_lens)\n",
    "    h0 = torch.tanh(model.bridge_h(h[-1])).unsqueeze(0)\n",
    "    c0 = torch.tanh(model.bridge_c(c[-1])).unsqueeze(0)\n",
    "    enc_mask = model.make_src_mask(src_lens, enc_outs.size(1))\n",
    "\n",
    "    inp = torch.tensor([bos_idx], device=src.device)\n",
    "    hidden = (h0, c0)\n",
    "    out_ids = []\n",
    "\n",
    "    for t in range(max_len - 1):\n",
    "        logits, hidden = model.decoder.forward_step(inp, hidden, enc_outs, enc_mask)\n",
    "        logits = logits / max(1e-6, temperature)\n",
    "        probs = F.softmax(logits, dim=-1).squeeze(0)\n",
    "\n",
    "        # block EOS until min_len\n",
    "        if t < min_len:\n",
    "            probs[eos_idx] = 0.0\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        # top-k\n",
    "        if top_k is not None and top_k > 0:\n",
    "            topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)))\n",
    "            mask = torch.ones_like(probs, dtype=torch.bool)\n",
    "            mask[topk_idx] = False\n",
    "            probs = torch.where(mask, torch.zeros_like(probs), probs)\n",
    "\n",
    "        # top-p (nucleus)\n",
    "        if top_p is not None and 0 < top_p < 1:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "            cutoff = cumsum > top_p\n",
    "            cutoff[0] = False\n",
    "            sorted_probs[cutoff] = 0\n",
    "            probs = torch.zeros_like(probs).scatter(0, sorted_idx, sorted_probs)\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        tok = torch.multinomial(probs, 1).item()\n",
    "        if tok == eos_idx:\n",
    "            break\n",
    "        out_ids.append(tok)\n",
    "        inp = torch.tensor([tok], device=src.device)\n",
    "\n",
    "    return [out_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5952a791-e00a-4ddb-b584-72afd11b1c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok I'll call back sunday evening forward evening.\n"
     ]
    }
   ],
   "source": [
    "_contraction_fixes = [\n",
    "    (r\"\\bdont\\b\", \"don't\"), (r\"\\bdoesnt\\b\", \"doesn't\"), (r\"\\bdidnt\\b\", \"didn't\"),\n",
    "    (r\"\\bhasnt\\b\", \"hasn't\"), (r\"\\bhavent\\b\", \"haven't\"), (r\"\\bhadnt\\b\", \"hadn't\"),\n",
    "    (r\"\\bcant\\b\", \"can't\"), (r\"\\bcouldnt\\b\", \"couldn't\"), (r\"\\bshouldnt\\b\", \"shouldn't\"),\n",
    "    (r\"\\bwouldnt\\b\", \"wouldn't\"), (r\"\\bwont\\b\", \"won't\"), (r\"\\bisnt\\b\", \"isn't\"),\n",
    "    (r\"\\baren't\\b\", \"aren't\"), (r\"\\bim\\b\", \"I'm\"), (r\"\\bi'm\\b\", \"I'm\"),\n",
    "    (r\"\\bive\\b\", \"I've\"), (r\"\\bi've\\b\", \"I've\"), (r\"\\bill\\b\", \"I'll\"), (r\"\\bi'll\\b\", \"I'll\"),\n",
    "    (r\"\\biyoull\\b\", \"you'll\"), (r\"\\bthatll\\b\", \"that'll\"), (r\"\\btheres\\b\", \"there's\"),\n",
    "    (r\"\\bwhats\\b\", \"what's\"), (r\"\\blets\\b\", \"let's\"),\n",
    "]\n",
    "def clean_generated_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    # tighten spacing before punctuation\n",
    "    text = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", text)\n",
    "    # apply contraction fixes (case-insensitive)\n",
    "    for pat, rep in _contraction_fixes:\n",
    "        text = re.sub(pat, rep, text, flags=re.IGNORECASE)\n",
    "    # tidy casing and terminal punctuation\n",
    "    text = text.strip()\n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:]\n",
    "        if text[-1] not in \".!?\":\n",
    "            text += \".\"\n",
    "    return text\n",
    "\n",
    "# encode context for inference (unchanged)\n",
    "def encode_context(text, emo_id=None, act_id=None, ctx_turns=1):\n",
    "    if isinstance(text, str):\n",
    "        turns = [text]\n",
    "    else:\n",
    "        turns = text[-ctx_turns:]\n",
    "\n",
    "    toks = []\n",
    "    if emo_id is not None: toks.append(f\"<EMO_{int(emo_id)}>\")\n",
    "    if act_id is not None: toks.append(f\"<ACT_{int(act_id)}>\")\n",
    "    for t in turns:\n",
    "        if len(toks) > 0: toks.append(\"<CTX_SEP>\")\n",
    "        toks.extend(preprocess_text(t))\n",
    "\n",
    "    ids = toks_to_ids(toks, max_src_len)\n",
    "    ids = pad_to(ids, pad_idx, max_src_len)\n",
    "    return torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_reply(context, emo_id=None, act_id=None, max_len=max_tgt_len,\n",
    "                   strategy=\"beam\",\n",
    "                   beam_size=5, length_penalty=0.8, min_len=6,\n",
    "                   temperature=0.9, top_k=50, top_p=0.9):\n",
    "    src = encode_context(context, emo_id, act_id).to(device)                   \n",
    "    src_len = torch.tensor([(src != pad_idx).sum().item()], device=device)     \n",
    "    if strategy == \"beam\":\n",
    "        pred_ids = beam_search_decode(model, src, src_len, max_len=max_len,\n",
    "                                      beam_size=beam_size, length_penalty=length_penalty,\n",
    "                                      min_len=min_len)[0]\n",
    "    else:\n",
    "        pred_ids = sample_decode(model, src, src_len, max_len=max_len,\n",
    "                                 temperature=temperature, top_k=top_k, top_p=top_p,\n",
    "                                 min_len=min_len)[0]\n",
    "    raw = decode_ids(pred_ids)\n",
    "    return clean_generated_text(raw)\n",
    "\n",
    "\n",
    "print(generate_reply(\n",
    "    [\"Could you send the file?\", \"Sure, I will send it by evening.\"],\n",
    "    emo_id=4,\n",
    "    act_id=1,\n",
    "    strategy=\"beam\",\n",
    "    beam_size=5,\n",
    "    length_penalty=0.8,\n",
    "    min_len=8\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b118e-4b09-4a92-93ea-22d1afac69dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_project)",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
